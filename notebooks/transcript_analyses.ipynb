{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84438d74-b84b-46e7-8781-e1e9cf01e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp': '0:07', 'text': \"Welcome Back To The Head heart and Boots podcast.\\nI'm Chris and I'm Brandon.\\nJoin us as we wrestle with what it takes to transform ourselves and the businesses we leave this industry.\\nHello.\\nHow are you?\", 'podcast_name': 'Head Heart  Boots  Ep 74   Who Are You Really'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def clean_filename(filename):\n",
    "    \"\"\"\n",
    "    Clean the podcast name by removing or replacing unwanted characters like underscores.\n",
    "    \"\"\"\n",
    "    cleaned_name = filename.replace('_', ' ')  # Replace underscores with spaces\n",
    "    cleaned_name = re.sub(r'[^\\w\\s]', '', cleaned_name)  # Remove any non-alphanumeric characters except spaces\n",
    "    return cleaned_name.strip()\n",
    "\n",
    "def split_document_by_timestamps(document, podcast_name):\n",
    "    # Regular expression to capture timestamps in the format of \"0:00\", \"1:10\", etc.\n",
    "    timestamp_pattern = r\"\\d{1,2}:\\d{2}\"\n",
    "    \n",
    "    # Find all the timestamps in the document\n",
    "    timestamps = re.findall(timestamp_pattern, document)\n",
    "    \n",
    "    # Split the document by timestamps\n",
    "    chunks = re.split(timestamp_pattern, document)\n",
    "    \n",
    "    # Remove any empty strings and pair each chunk with its timestamp\n",
    "    parsed_document = []\n",
    "    for i in range(len(timestamps)):\n",
    "        parsed_document.append({\n",
    "            'timestamp': timestamps[i],\n",
    "            'text': chunks[i + 1].strip(),  # Remove leading/trailing whitespace\n",
    "            'podcast_name': podcast_name  # Add the cleaned podcast name field\n",
    "        })\n",
    "    \n",
    "    return parsed_document\n",
    "\n",
    "# Function to process all text files in a given folder\n",
    "def process_all_podcasts_in_folder(folder_path):\n",
    "    all_parsed_documents = []\n",
    "    \n",
    "    # Loop through each file in the specified folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a text file (you can adjust this condition if needed)\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            podcast_name = os.path.splitext(filename)[0]  # Get the file name without extension\n",
    "            podcast_name = clean_filename(podcast_name)  # Clean the podcast name\n",
    "            \n",
    "            # Read the content of the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                document = file.read()\n",
    "            \n",
    "            # Split the document and get the parsed data\n",
    "            parsed_document = split_document_by_timestamps(document, podcast_name)\n",
    "            \n",
    "            # Add the parsed document data to the final list\n",
    "            all_parsed_documents.extend(parsed_document)\n",
    "    \n",
    "    return all_parsed_documents\n",
    "\n",
    "# Specify the folder containing the podcast transcripts\n",
    "folder_path = '../resources'  # Replace this with the actual folder path\n",
    "\n",
    "# Process all files in the folder\n",
    "all_podcasts_data = process_all_podcasts_in_folder(folder_path)\n",
    "\n",
    "# Output the result (you can further process this or save it to a file)\n",
    "print(all_podcasts_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143cf02-d7f6-493c-9bc2-5840f67bcce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095d3c04-8fc9-4419-b032-d9bfc3325563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_document = all_podcasts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51cd0c0f-4649-46ea-a207-711fcedf3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing weavite and openai dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "faddd614-49d0-4267-b582-394b95f3a6ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (0.28.0)\n",
      "Requirement already satisfied: weaviate-client in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (4.8.1)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai==0.28) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai==0.28) (3.10.8)\n",
      "Requirement already satisfied: httpx<=0.27.0,>=0.25.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (0.27.0)\n",
      "Requirement already satisfied: validators==0.34.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (0.34.0)\n",
      "Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (1.3.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (2.9.2)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from weaviate-client) (1.66.2)\n",
      "Requirement already satisfied: cryptography in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.1)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client) (5.28.2)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (74.0.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.8)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpcore==1.*->httpx<=0.27.0,>=0.25.0->weaviate-client) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.13.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U openai==0.28 weaviate-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68726752-16f6-4f2e-a5f9-a8fd80fd8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a979033-519d-438f-997b-0bca4ee639f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai==1.51.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=25164) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze | grep openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d1e4fa-a0b5-44e8-8410-9e2417983064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60c3df-2673-4dab-8649-58bbc173129b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c245f292-3c66-46e0-b1f3-e6a5bcbe1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd6f4d68-5d53-4a51-8540-8f61ae389732",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_custom(\n",
    "            http_host=\"localhost\",\n",
    "            http_port=8080,\n",
    "            http_secure=False,\n",
    "            grpc_host=\"localhost\",\n",
    "            grpc_port=50051,\n",
    "            grpc_secure=False,\n",
    "            headers={\n",
    "                \"X-OpenAI-Api-Key\": OPENAI_API_KEY\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757b35c5-93cb-4634-8914-18858f026d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daef12db-5a09-4100-8081-889cd8d367c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/selector_events.py:879: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=104 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Creating embedings with openai\n",
    "def get_embeddings(text, model=\"text-embedding-3-large\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai_client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "for chunk in parsed_document:\n",
    "    chunk['embedding'] = get_embeddings(chunk['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43f18fe9-6a3a-4069-9543-10efa37df028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gw/jc9bnsh52z51skt515yl1lb80000gn/T/ipykernel_25164/1471486422.py:4: DeprecationWarning: \n",
      "Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  weaviate_client = weaviate.Client(\"http://localhost:8080\")  # Change the URL to your Weaviate instance\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "# Initialize the Weaviate client\n",
    "weaviate_client = weaviate.Client(\"http://localhost:8080\")  # Change the URL to your Weaviate instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21b4ab69-d06e-432f-8eac-8c70641a7c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"PodcastChunk\" already exists'}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the schema if not already set up\u001b[39;00m\n\u001b[1;32m      2\u001b[0m class_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPodcastChunk\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m \u001b[43mweaviate_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/weaviate/schema/crud_schema.py:253\u001b[0m, in \u001b[0;36mSchema.create_class\u001b[0;34m(self, schema_class)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mCreate a single class as part of the schema in Weaviate.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    If the 'schema_class' could not be validated against the standard format.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m loaded_schema_class \u001b[38;5;241m=\u001b[39m _get_dict_from_object(schema_class)\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_class_with_primitives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_schema_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_complex_properties_from_class(loaded_schema_class)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/weaviate/schema/crud_schema.py:817\u001b[0m, in \u001b[0;36mSchema._create_class_with_primitives\u001b[0;34m(self, weaviate_class)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsConnectionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass may not have been created properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconn_err\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate class\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m: Create class! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"PodcastChunk\" already exists'}]}."
     ]
    }
   ],
   "source": [
    "# Define the schema if not already set up\n",
    "class_schema = {\n",
    "    \"class\": \"PodcastChunk\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"timestamp\",\n",
    "            \"dataType\": [\"string\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"dataType\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"podcast_name\",\n",
    "            \"dataType\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"dataType\": [\"number[]\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "weaviate_client.schema.create_class(class_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34b792-bb9a-4f7e-9ebb-314ba27609d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c783cd-f0f3-4b17-abb3-582f567e2a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcollection = weaviate_client.collections.get(\"PodcastChunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7c719e-e935-440a-8597-47a018507746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x1097caba0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb5cf21-03a0-41eb-84bd-262a7f7c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in parsed_document:\n",
    "    wcollection.data.insert(\n",
    "        properties={\n",
    "            \"timestamp\": chunk['timestamp'],\n",
    "            \"text\": chunk['text'],\n",
    "            \"podcast_name\": chunk['podcast_name']\n",
    "        },\n",
    "        vector=chunk['embedding']\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06f139b8-0b69-43c6-b14f-f5ce988451d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def _retrieve_transcripts( query: str, n: Optional[int] = 20):\n",
    "        query_embedding = openai_client.embeddings.create(input=[query], model='text-embedding-3-large').data[0].embedding\n",
    "        results = (\n",
    "            weaviate_client.query\n",
    "            .get(\"PodcastChunk\", [\"timestamp\",  \"text\", \"podcast_name\"])\n",
    "            .with_hybrid(\n",
    "                query=query,  # Replace with relevant search term\n",
    "                vector=query_embedding,\n",
    "                alpha=0.75  # Adjust this value to balance between vector and keyword search\n",
    "            )\n",
    "            .with_limit(n)  # Increased limit for more results\n",
    "            .with_additional([\"score\"])  # Include the score to see relevance\n",
    "            .do()\n",
    "        )\n",
    "        return results['data']['Get']['PodcastChunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6905872e-226d-4feb-b9e2-7883ad1df86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_additional': {'score': '0.8291863'},\n",
       "  'text': \"And it can do anything from helping a new team member assimilate some estimating Beck's practices.\\nAnd it also helps the grizzled vets add back that few percent that we've just forgot over time.\\nSo, Actionable Insights.\\nGet insights.org/floodlight and take a look at what the Actionable Insights Exactimate profile could be doing for you and your team.\",\n",
       "  'timestamp': '41:13'},\n",
       " {'_additional': {'score': '0.75'},\n",
       "  'text': \"That's the game changer.\\nIt's essentially an AI tool that's walking alongside of you as you write your estimate, bringing things to your attention that should be added that could be considered all of them.\\nItems that increase our profitability, increase the effectiveness and the consistency of that scope.\",\n",
       "  'timestamp': '40:55'},\n",
       " {'_additional': {'score': '0.69543576'},\n",
       "  'text': \"Maybe that one project was 50% of their net for the year.\\nDo you think that changes the discussion?\\nIt absolutely does.\\nSo, and this is just the tip of the iceberg.\\nIt can be any number of variables.\\nBut does that help Chris?\\nOh yeah.\\nNot to be crude, but the most common phrase that we hear from clients when they're in that due diligence phase is they have crawled up my ass, right?\",\n",
       "  'timestamp': '37:51'},\n",
       " {'_additional': {'score': '0.5596664'},\n",
       "  'text': \"And the unpredictability too of the cost of mobilization.\\nSome of those opportunities don't come to fruition and you have these significant costs that are just sunk cost.\\nSo there's that whole cat travel hurricane type of.\\nThen there's the local organic cultivating direct sales relationships with small hotel groups, local school districts, that kind of stuff.\",\n",
       "  'timestamp': '15:22'},\n",
       " {'_additional': {'score': '0.5248125'},\n",
       "  'text': \"And when they can't do it, their default position is guess we'd better rely on something that's more predictable.\\nSo they've got to sell it and it's difficult to sell I.\\nThink in the midst of this conversation, we're really talking about a few different things, right?\\nLike there's the the weather related catastrophe losses, which often is commercial hotels, schools, commercial buildings, etcetera, which big high dollar, big high margin and also long AR collection periods and so forth.\",\n",
       "  'timestamp': '14:51'},\n",
       " {'_additional': {'score': '0.4985532'},\n",
       "  'text': \"So and.\\nI think too, and we've had to, as much as we can get caught up in that too.\\nWe've had to remind our clients of the same thing when they start getting into a conversation with a strategic or APE company or whatever and they start to do the napkin math and it's like, Oh my God.\\nEvery.\\nTen of those conversations, 56789 of them, end up the numbers look dramatically different when they actually come to the altar, right?\",\n",
       "  'timestamp': '31:56'},\n",
       " {'_additional': {'score': '0.45093852'},\n",
       "  'text': \"So we can't, we can't sugarcoat some things because we need a comfortable buyer or we don't have a deal.\\nSo the numbers, the owner involvement, the client risk factors, the cyclicality, you know, we typically go, we, let me say it this way, every buyer will pull monthly reports for the last 24 months.\",\n",
       "  'timestamp': '37:08'},\n",
       " {'_additional': {'score': '0.401322'},\n",
       "  'text': 'Devastating.\\nIt changes the whole conversation.\\nAnd so I go to great lengths to make sure that they were spot on.\\nOK, So the types of issues that come up in due diligence, first and foremost, is this QE OK, There are issues that arise are are there, you know, do they need you, the owner and what are they going to pay you?',\n",
       "  'timestamp': '35:40'},\n",
       " {'_additional': {'score': '0.40018445'},\n",
       "  'text': \"And I think if you're cool with it, I think one of the things that I would love to lean into is wrong assumptions, right?\\nLike normally as business owners, we think it's going to be this thing that's going to make all the difference.\\nI think often we're wrong.\\nSo just leaning into that from your perspective and let's see where where it takes us.\",\n",
       "  'timestamp': '17:47'},\n",
       " {'_additional': {'score': '0.3992314'},\n",
       "  'text': \"We definitely know of people, their number that has part of an LOI at this point is people are coming back to the table and saying not so fast.\\nWe're not going to be able to stick with that, right?\\nWhat do we do?\\nLike what are you seeing?\\nWhat are some of the patterns?\",\n",
       "  'timestamp': '17:31'},\n",
       " {'_additional': {'score': '0.3970363'},\n",
       "  'text': \"We just don't realize it, right?\\nI mean, most companies monies out there can benefit not a little bit, a lot by using a consultant.\\nAnd there's blind spots that we all have in our business.\\nAnd, and some people don't even know that there's blind spots because they don't even acknowledge them.\",\n",
       "  'timestamp': '50:15'},\n",
       " {'_additional': {'score': '0.3951791'},\n",
       "  'text': \"Correct.\\nWhat what are some of the hot buttons that start to erode value when that banker, PE, company perspective owner or buyer to look at things that really tend to pop up?\\nYeah, it's a perfect question and it's a loaded one.\",\n",
       "  'timestamp': '33:00'},\n",
       " {'_additional': {'score': '0.3895831'},\n",
       "  'text': \"Would you agree with that?\\nAt least that is correct.\\nYeah, first pass.\\nOh, they have a lot of commercial, right.\\nThey don't necessarily differentiate those two things.\\nYeah, large loss and some of the cat looks great on paper, but you get down to the nuts and bolts of it.\\nI mean, I've had clients wait 2 1/2 years for FEMA approval on 1,000,006 project.\",\n",
       "  'timestamp': '16:40'},\n",
       " {'_additional': {'score': '0.3850451'},\n",
       "  'text': \"Some are cleaner than others.\\nSome have client risk factors, some have ownership factors, some have key employee factors, some have cyclicality, others There's any number of reasons why there may be risk.\\nBut an earn out levels to the playing field.\",\n",
       "  'timestamp': '22:13'},\n",
       " {'_additional': {'score': '0.37301743'},\n",
       "  'text': \"And everyone defines material differently.\\nAnd so when it comes to, I always challenge owners to say, if it's material to you, it's definitely material to them.\\nAnd they're going to find 50 more items that are material to them that aren't material to you.\",\n",
       "  'timestamp': '59:13'},\n",
       " {'_additional': {'score': '0.36104187'},\n",
       "  'text': \"And what happens so often these companies go to the market with or they entertain these conversations with just some financials and there's no discussion about odor involvement, no discussion about client risk factors, no discussion about TP, ES and their effects and no discussion about AR and collectibles and those sorts of things.\",\n",
       "  'timestamp': '33:57'},\n",
       " {'_additional': {'score': '0.36024076'},\n",
       "  'text': \"What are the foundational things that we need to be building out, preparing or leaning into so that we don't get blindsided with deductions later?\\nWell.\\nLet me start here from 100,000 feet.\\nThe financial story in the numbers have to match the story that a buyer's being told.\",\n",
       "  'timestamp': '55:02'},\n",
       " {'_additional': {'score': '0.35901728'},\n",
       "  'text': \":37\\nThe other thing that we see first hand is it is grueling to work on process when your business is already left the shipyard.\\nIt's grueling.\\nAnd we think that we're going to come to an experience and start dedicating sixty, 7080% of our time of quotes working on the business.\",\n",
       "  'timestamp': '1:07'},\n",
       " {'_additional': {'score': '0.3556174'},\n",
       "  'text': \":28\\nAnd here's the, I would just hammer on this too, is for some of you with larger business, this is really obvious.\\nBut for a lot of entrepreneurs, we just, if there's cash in the bank, we're feeling pretty good about what we're doing.\\nAnd the reality of it is, is when our hair is on fire and we're doing the thing, it can be hard to slow down and prioritize these things that are very hard for you and I to create value around.\",\n",
       "  'timestamp': '1:01'},\n",
       " {'_additional': {'score': '0.34528786'},\n",
       "  'text': \"And you can imagine the sellers, Whoa, whoa, whoa.\\nThat's the kind of best case scenario.\\nWe, we don't want to just set it at 45%.\\nWhat if we hit 20 or 25%?\\nWell, now the whole conversation changes.\\nSo the earn outs are a really tricky thing to navigate.\\nBut at the end of the day, I want the value to be delivered today and future value to be delivered in the future.\",\n",
       "  'timestamp': '23:07'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_retrieve_transcripts(\"What are common challenges faced in estimating?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72d01809-9437-4d0b-b088-f882a3bbd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c96fba0a-3802-4250-a3ad-3838c367f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain_openai in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (0.2.2)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (0.1.131)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain_openai) (1.51.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain_openai) (0.8.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 langchain_community-0.3.1 marshmallow-3.22.0 mypy-extensions-1.0.0 pydantic-settings-2.5.2 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_openai langchain_community\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3f6afa57-f8b6-4e9b-80f3-cab93c124c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/pydantic/_internal/_config.py:291: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, Any, List, Optional\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import json\n",
    "\n",
    "\n",
    "TRANSCRIPT_RETRIEVAL_SYSTEM_PROMPT = \"\"\"You are an AI assistant tasked with retrieving, interpreting, and presenting information from podcast transcript to answer a user's question. Your goal is to provide a detailed answer to user requests.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "\n",
    "1. NEVER make up or infer information not present in the provided transcripts.\n",
    "2. If the transcript don't provide all the information needed to answer the question fully, clearly state what's missing.\n",
    "\n",
    "Guidelines for retrieval:\n",
    "\n",
    "1. Analyze all the provided transcripts carefully. \n",
    "\n",
    "Your response should help the user understand not only the content of the transcript but also the reasoning behind the selection of each transcript, providing a comprehensive and interpretable overview of the available information.\"\"\"\n",
    "\n",
    "RETREIVAL_FORMAT_PROMPT = \"\"\"\n",
    "You are an AI language model designed to transform user requests into optimized search queries for a Weaviate index of poscast transcripts, utilizing vector-based search powered by OpenAI embeddings.\n",
    "\n",
    "Your Task:\n",
    "\n",
    "Given a user request:\n",
    "\n",
    "Identify Terms: Extract all terms mentioned in the transcripts.\n",
    "\n",
    "Expand Concepts: For each term, generate related aspects.\n",
    "\n",
    "Compose a Descriptive Query: Create a concise and informative sentence or phrase that encompasses the term and its related aspects, effectively describing the kind of transcript being searched for.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "Content: Include key aspects of the term to aid in retrieving relevant transcripts.\n",
    "Style: Write in a tone typical for podcasts.\n",
    "Relevance: Ensure the query reflects the user's interest in the podcasts and related.\n",
    "Conciseness: Keep the query short and focused.\n",
    "Example:\n",
    "\n",
    "Only return the formatted query, nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def parse_llm_output(response: str):\n",
    "    \"\"\"\n",
    "    Parse the LLM output into HTML and summary parts.\n",
    "    \"\"\"\n",
    "    # Split the response into HTML and summary parts\n",
    "    parts = re.split(r'(</html>)', response, maxsplit=1)\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        html_content = parts[0] + parts[1]\n",
    "        summary = parts[2].strip() if len(parts) > 2 else \"\"\n",
    "    else:\n",
    "        html_content = \"\"\n",
    "        summary = response.strip()\n",
    "    \n",
    "    return html_content, summary\n",
    "\n",
    "def clean_html(html_content: str):\n",
    "    \"\"\"\n",
    "    Clean and format the HTML content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Remove comments\n",
    "    comments = soup.findAll(text=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Ensure the table has border and cellpadding\n",
    "    table = soup.find('table')\n",
    "    if table:\n",
    "        table['border'] = '1'\n",
    "        table['cellpadding'] = '5'\n",
    "    \n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def format_retreival_query(llm: BaseLanguageModel, user_query: str) -> str:\n",
    "\n",
    "    human_message = f\"Format this user query for articles retreival: {user_query}\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", RETREIVAL_FORMAT_PROMPT),\n",
    "        (\"human\", human_message),\n",
    "    ])\n",
    "\n",
    "    response = llm(prompt.format_messages())\n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "class TranscriptRetrievalChain(Chain):\n",
    "    llm: BaseLanguageModel\n",
    "    weaviate_client: Any\n",
    "    openai_client: Any\n",
    "    cl_instance: Any\n",
    "    alpha: float = 0.75\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"question\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"answer\"]\n",
    "\n",
    "    def sort_by_timestamp(self, results: List[Dict]) -> List[Dict]:\n",
    "        def timestamp_to_seconds(timestamp: str) -> int:\n",
    "            minutes, seconds = map(int, timestamp.split(':'))\n",
    "            return minutes * 60 + seconds\n",
    "        return sorted(results, key=lambda transcript: timestamp_to_seconds(transcript['timestamp']))\n",
    "\n",
    "    def _retrieve_transcripts(self, query: str, n: int = 20, rich_author_meta=False) -> List[Dict]:\n",
    "\n",
    "        query_formatted = format_retreival_query(self.llm, query)\n",
    "\n",
    "        print(\"Formatted query: \", query_formatted)\n",
    "        \n",
    "        #self.cl_instance\n",
    "        \n",
    "\n",
    "        query_embedding = self.openai_client.embeddings.create(input=[query_formatted], model='text-embedding-3-large').data[0].embedding\n",
    "\n",
    "        fields = [\"timestamp\", \"text\", \"podcast_name\"]\n",
    "    \n",
    "        results = (\n",
    "            self.weaviate_client.query\n",
    "            .get(\"PodcastChunk\", fields)\n",
    "            .with_hybrid(\n",
    "                query=query,\n",
    "                vector=query_embedding,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            .with_limit(n)\n",
    "            .with_additional([\"score\"])\n",
    "            .do()\n",
    "        )\n",
    "\n",
    "        return results['data']['Get']['PodcastChunk']\n",
    "\n",
    "    def transcript_synthesize(self, results: List[Dict], original_question: str, task: str) -> str:\n",
    "        system_prompt = TRANSCRIPT_RETRIEVAL_SYSTEM_PROMPT\n",
    "        \n",
    "        transcripts = results\n",
    "\n",
    "        # Format the transcripts for the prompt\n",
    "        formatted_transcripts = json.dumps(transcripts, indent=2)\n",
    "        human_message_template = \"\"\"Original question: {question}\n",
    "        PodcastChunk:\n",
    "        {transcripts}\n",
    "        Please {task} the information from these transcripts to answer the original question.\"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", human_message_template),\n",
    "        ])\n",
    "\n",
    "        # import pdb; pdb.set_trace()\n",
    "\n",
    "        response = self.llm(prompt.format_messages(\n",
    "            question=original_question,\n",
    "            transcripts=formatted_transcripts,\n",
    "            task = task,\n",
    "        ))\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        question = inputs[\"question\"]\n",
    "        task = inputs.get(\"task\", \"summary\")  # Default to summary if task is not provided\n",
    "        \n",
    "        # Retrieve transcripts\n",
    "        results = await asyncio.to_thread(self._retrieve_transcripts, question)\n",
    "        \n",
    "        results_filtered = await asyncio.to_thread(self.sort_by_timestamp, results)\n",
    "       \n",
    "        # Synthesize final answer\n",
    "        final_answer = await asyncio.to_thread(self.transcript_synthesize, results_filtered, question, task)\n",
    "        \n",
    "        return {\"answer\": final_answer}\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None\n",
    "    ) -> Dict[str, str]:\n",
    "        question = inputs[\"question\"]\n",
    "        task = inputs.get(\"task\", \"summary\")  # Default to summary if task is not provided\n",
    "\n",
    "        # Retrieve articles\n",
    "        results = self._retrieve_transcripts(question)\n",
    "       \n",
    "        # Synthesize final answer\n",
    "        final_answer = self.transcript_synthesize(results_filtered, question, task)\n",
    "        \n",
    "        return {\"answer\": final_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b47892ca-ebd4-493e-8a29-07963e8b5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import weaviate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "class TranscriptAgent:\n",
    "    def __init__(self, cl_instance=None, alpha=.75):\n",
    "        self.openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        self.weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "        # self.cl_instance = cl_instance\n",
    "        # self.logger = setup_logging()\n",
    "        # self.logger.info(\"Transcipt initialized\")\n",
    "\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "        \n",
    "        self.transcript_chain = TranscriptRetrievalChain(\n",
    "            llm=self.llm,\n",
    "            weaviate_client=self.weaviate_client,\n",
    "            openai_client=self.openai_client,\n",
    "            cl_instance=cl_instance,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        \n",
    "\n",
    "    async def route_and_execute(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
    "        question = inputs[\"question\"]\n",
    "        result = await self.transcript_chain.ainvoke({\"question\": question, \"task\": \"retrieve\"})\n",
    "        \n",
    "        print(f\"Query: {question}\")\n",
    "        print(f\"Response: {result['answer']}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    async def process_query(self, query: str, user_id: str) -> str:\n",
    "        self.memory.chat_memory.add_user_message(query)\n",
    "        result = await self.route_and_execute({\"question\": query})\n",
    "        answer = result.get(\"answer\", \"I'm sorry, I couldn't generate a response for this query.\")\n",
    "        print(f\"User {user_id} - Query: {query}\")\n",
    "        print(f\"User {user_id} - Response: {answer}\")\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d61645-66f3-41d6-a690-ebe177ac908c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8d3ffa86-7e29-4ac8-9970-40509db76b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up chain and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e8135dd0-ff35-4f3d-b13d-e973c06d99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gw/jc9bnsh52z51skt515yl1lb80000gn/T/ipykernel_69213/3037426159.py:12: DeprecationWarning: \n",
      "Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  self.weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated and will\n",
      "            be removed by 2024-11-30.\n",
      "\n",
      "            Upgrade your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "                - For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "                - For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "\n",
      "            If you have to use v3 code, install the v3 client and pin the v3 dependency in your requirements file: `weaviate-client>=3.26.7;<4.0.0`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = TranscriptAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "055cb730-df6e-42c9-a7af-9ec2a6f81b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted query:  Exploring roles like project managers, cost estimators, and quantity surveyors involved in the estimating process.\n",
      "Query: Which roles typically deal with the estimating process?\n",
      "Response: The provided podcast transcripts do not explicitly mention which roles typically deal with the estimating process. However, there are some relevant insights that can be inferred:\n",
      "\n",
      "1. **Department Heads and GMs**: One transcript mentions department heads and general managers (GMs) becoming part of the production cycle, which could imply involvement in processes like estimating, especially if it relates to production efficiency and cost management (timestamp: 1:07).\n",
      "\n",
      "2. **Finance Directors**: The same transcript also mentions finance directors, who are likely involved in the estimating process due to their role in managing financial aspects, including cost estimation and budgeting (timestamp: 1:07).\n",
      "\n",
      "3. **Chief Engineer or Director of Facilities**: Another transcript mentions creating relationships with a chief engineer or director of facilities, which suggests these roles might be involved in estimating processes related to engineering or facility management projects (timestamp: 16:21).\n",
      "\n",
      "4. **AI Tools and Estimating**: There is a mention of an AI tool that assists in the estimating process by bringing attention to items that could increase profitability and consistency (timestamp: 40:55). This implies that roles involved in estimating might also interact with such tools to enhance their processes.\n",
      "\n",
      "5. **New Team Members and Experienced Estimators**: The transcript also refers to new team members assimilating estimating best practices and experienced estimators (referred to as \"grizzled vets\") refining their estimates, indicating that both new and experienced team members are involved in the estimating process (timestamp: 41:13).\n",
      "\n",
      "Overall, while the transcripts provide some context, they do not explicitly list the roles typically involved in the estimating process. For a more comprehensive answer, additional information or context from the podcast would be necessary.\n",
      "User 1 - Query: Which roles typically deal with the estimating process?\n",
      "User 1 - Response: The provided podcast transcripts do not explicitly mention which roles typically deal with the estimating process. However, there are some relevant insights that can be inferred:\n",
      "\n",
      "1. **Department Heads and GMs**: One transcript mentions department heads and general managers (GMs) becoming part of the production cycle, which could imply involvement in processes like estimating, especially if it relates to production efficiency and cost management (timestamp: 1:07).\n",
      "\n",
      "2. **Finance Directors**: The same transcript also mentions finance directors, who are likely involved in the estimating process due to their role in managing financial aspects, including cost estimation and budgeting (timestamp: 1:07).\n",
      "\n",
      "3. **Chief Engineer or Director of Facilities**: Another transcript mentions creating relationships with a chief engineer or director of facilities, which suggests these roles might be involved in estimating processes related to engineering or facility management projects (timestamp: 16:21).\n",
      "\n",
      "4. **AI Tools and Estimating**: There is a mention of an AI tool that assists in the estimating process by bringing attention to items that could increase profitability and consistency (timestamp: 40:55). This implies that roles involved in estimating might also interact with such tools to enhance their processes.\n",
      "\n",
      "5. **New Team Members and Experienced Estimators**: The transcript also refers to new team members assimilating estimating best practices and experienced estimators (referred to as \"grizzled vets\") refining their estimates, indicating that both new and experienced team members are involved in the estimating process (timestamp: 41:13).\n",
      "\n",
      "Overall, while the transcripts provide some context, they do not explicitly list the roles typically involved in the estimating process. For a more comprehensive answer, additional information or context from the podcast would be necessary.\n"
     ]
    }
   ],
   "source": [
    "response = await agent.process_query(\"Which roles typically deal with the estimating process?\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc7731-0bf1-46bb-9087-90c4582149bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7f6b8-93e3-4cc9-8edd-f01f152b96e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036113b4-f5ed-4481-9a2d-4a1953611c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71904488-afd7-47ee-bc80-eb731a334ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
